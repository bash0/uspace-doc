\chapter{Imaging and Tracking Payload Unit}
\label{chap:itpu}

The Imaging and Tracking Payload Unit (ITPU) is the scientific payload of the
airship, and is in general independent of the airship's control system, but uses
the airships power system. However it would be possible in an extended version
to also incorporate a controlling interface and connect the motor ESCs for the
motor control to the computer of the ITPU. 
\\
\\
The purpose of the ITPU is to take aerial images from different positions, acquire
accurate position and attitude data and use those combined information to create
aerial image maps. 
\\
\\
The system is further divided into the following parts:
\begin{itemize}
\item Attitude determination: Usage of advanced data fusion method to facilitate
GPS, Gyro, Accelerometer and Magnetometer information, to extract accurate
position and attitude information together with reasonable error estimates.
\item Imaging system: A megapixel resolution webcam will provide images in
regular time-steps in the order of a second.
The image data will be saved on a SD memory card together with attitude
information for off-line processing.
\item Communication system: Attitude data and spacecraft telemetry will be
transmitted to ground.
\item Image processing software:
Image matching and evaluation will be done on a standard PC after payload
recovery.
\end{itemize}

\section{Functional and Technical Requirements}

\begin{itemize}
\item Measure absolute and accurate position and pointing angles.
\item Take images in regular steps and save them together with attitude data.
\item Receive and execute basic telecommands such as image capture start/stop.
\item Send basic telemetry data such as position.
\item Combine single image captures to a large area map.
\item Operate in open air environment up to 500~m over ground. In U-Space the
module will only fly to a height of a few tens of meters. However it should be
used in higher altitudes for later applications.
\item Operate at 5~V unstabilized input voltage at a maximum power consumption
of 2.5~W.
\item Store at least 1000 medium-resolution images.
\end{itemize}


\section{Electronic Components}

\begin{itemize}
 \item Board computer
 \item Accelerometer
 \item Magnetometer
 \item Gyroscope
 \item GPS-receiver
 \item Transmitter/Receiver
 \item Camera
\end{itemize}

\subsection*{Board Computer}

For reading the sensors, communicating with the ground station and saving
images from the camera an embedded system which provides all necessary
interfaces and enough computing power to handle comparably large data streams
from the camera was needed. It was chosen to use the BeagleBone, but only the
BeagleBoard was delivered which is a related predecessor and lacks some
features.

\pagebreak

\noindent
The BeagleBoard is a microcontroller board running a TI OMAP3530 ARM Cortex-A8
system on a chip (SoC). It provides 256~MB RAM and several high and low level
interfaces. For this project the main interfaces used are the I2C and UART for
communication with the sensors (gyroscope, magnetometer, accelerometer) and
the GPS-receiver and radio-transmitter module (E-TAG) respectively. The camera
The used high level interface is the USB-host adapter to connect the camera
to. The operating system with the control program as well as the image files
are stored onto an 8~GB class 10 sd-card. The needed supply voltage for the BeagleBoard is 5~V. The voltage of the pins on the expansion header is 1.8~V.

\subsection*{Sensors}

For determining the position and attitude the LSM303DLM \cite{LSM303:datasheet}
combined magnetometer and accelerometer and the ITG-3200 triple-axis gyroscope
\cite{ITG-3200:datasheet} from sparkfun were used. Both sensors have been used
during the CanSat-project in WÃ¼rzburg with decent results. They communicate
via an I2C interface with the main board at 3.3~V. For receiving GPS
information an E-TAG device developed by Esrange was
used. It is connected via a serial line interface with the main board. It also
provides a transmitter module for communicating with the ground station.

\subsection*{Camera}

As high quality embedded industrial cameras are very high priced, a consumer
webcam with a resolution of several megapixels will be connected to the USB-port
of the main board. It is intended to buy a camera which is supported by the
Linux operating system running on the main board.

%\subsection{Attitude Determination System}

\FloatBarrier
\section{Software Environment}

The BeagleBoard is equipped with an 8~GB sd-card. A Debian based Linux
operation system is installed onto the card providing the environment,
libraries and drivers for the program to run. As compared to the BeagleBone the
BeagleBoard lacks pull-up resistors on its I2C interface which produced
problems with Linux kernel the standard kernel for this board came without the
possibility to enable the I2C controller. Therefore an own version of the Linux
kernel had to be compiled for the BeagleBoard in order activate the I2C device
on the expansion header. 
\\
\\
The program is divided into several threads. One thread is responsible to get
the newest sensor data from the accelerometer, magnetometer and gyroscope
from the I2C interface and fuse them to accurate position and attitude
representations. It is running at a comparably high frequency (>50 Hz, the final
frequency is not defined yet).  A second thread is polling for new GPS data from
the E-TAG via serial line. As the GPS-chip only updates the positional data
around once per second this thread can run at a much lower frequency.
A third thread is responsible to control the camera. If it is active it will
shoot a picture every second and save it to the SD-card. And finally two
threads will be handling sending telemetry data to and receiving basic commands
from the ground station.
\\
\\
Due to lack of time a standard kernel was used which is not hard real-time
capable. Nevertheless it still has the possibility to use pre-emptive scheduling
which should give a high enough accuracy, but it should be noted that in future
developments it could be beneficial to use a real-time Linux kernel. 

\FloatBarrier
\section{Image Processing}
Provided sensoric and computational system (USpace - ITPU) is ment to be a craddle for image 
processing experiments that simulate Low Earth Orbit conditions. Given the metainformation 
available with images taken we can assume several basic scenarios,
\begin{itemize}
\item Aerial map creation (optionally automatic)
\item 3D reconstruction of large areas
\item Object tracking
\item Visual attitude determination
\end{itemize}
We will briefly cover the basic design of each possibility and it's scientific potential.

\subsection{Camera calibration}
We are using a simple and low cost camera, nevertheless, we have to calibrate the system and compensate
for projection distortion. Since we are assuming to take pictures at larger distances then the camera was designed for
this distortion compensation is crucial for proper quality output. We model the camera parameters assuming a simple
pin hole camera. We need to take pictures of a well known pattern with easily detectable lines or points on lines. 
Taking various pictures of a tilted chess board pattern will allow us to stably detect corners of the pattern. The corners
are lying on a straight line (under a rotation-translation transformation). Using this information we are able to calculate
the so called intrinsic camera parameters. As the straight lines of points will appear to be curved on the pictures
we can compute a transformation to straighten up these lines and therefore compensate for the lens distortion. 
There might be a need for future extrinsic camera calibration as well. This is importnat specially for 3D and automated 
applicaitons where a coordinate frame is needed.

\subsection{Preprocessing}
\label{sec:preprocessing}
A crucial step for quality assurance is the first preprocessing and data homologization step. We need our
input picture to be as clear and dynamically stable as possible as well as distortion free. There might be 
more steps involved but the most important are lens distortion compensation, contrast and brightness stabilization 
and blur removal. Lens distortion compensation is a compensation of lense's radial character and comes
as the calibration result. Contrast and brightness might be adjusted globally to avoid saturated or
noisy pictures. We have to assume that the aerial carrier is on the move and with not suitable light
conditions the image can get blurry. To compensate for this blur we need to deploy a motion blur
removal filter. Assuming linear and constant motion we can implement a Wiener filter. The motion
can be estimated from the sensorics part or image analysis. 

\subsection{Aerial map creation (optionally automatic)}
Creating an aerial map or stitching partial views into a single pictures is a basic space-related opeartion. Our implemented
approach consists of these steps,
\begin{enumerate}
\item Preprocessing
\item Feature detection
\item Feature rejection
\item Transformation estimation
\item Transformation
\item Image merging
\end{enumerate}
As preprocessing was described in section ~\ref{sec:preprocessing} we move on to the feature detection. As there
are many feature detectors this is the possibility to evaluate the performance of different feature detectors on different
climates or terrain types. We have decided to implement three feature extraction methods for the starters. The well
known SIFT keypoints, Kanade Lucas Tomasi (KLT) tracker based features and AGAST - a machine generated corner detector
developed at DLR for robust 3D reconstruction. 

Feature rejection then filters wrong, not relevant to the transformation we are looking for, or just bad features, not 
persistent in the pictures long enough. 

Based on the geometry of detected features and sensorics information we can estimate an affine transformation parameters
at minimum. All of these methods should provide enough control points so that we would be able to estimate even higher 
polynomial transformation to ease the image stitching at the edges.

After carrying out the transformation we can merge the pictures together with small overlapping regions around the edges
and make a smooth transition from picture to picture. This process can be handled by our processing unit on the fly so the
ITPU is able to provide a ready-made result.

\subsection{3D reconstruction of large ojects}
Another potential application is in robust 3D object reconstruction. Having multiple arbitrary views of one large object as for example a castle
we are able to find regions that are invariant to affine transformation. Having these regions we are able to calculate camera's position
and paramteres as well as transformation of the key points. This solution is based on MSER algorithm proposed by ... and imlpemented 
in OpenCV.

\subsection{Object tracking}
In case of ITPU having control over the steering of air carrier we could implement a real time object tracking system.
Tracking is a modified recognition problem which focuses on real-time performance. We assume a continuous target
movement which allows us to encorporate filtering and movement prediction methods such as Kalman filter. 

\subsection{Visual attitude determination}
Another importnat possibility of usage is attitude determination. From the relative transformation in between images a relative
6 degrees of freedom pose change can be estimated. Having samples of absolute reference this pose can be cleaned
of incremental errors and a long time solution can be tested. Such algorithm can be used for planetary and interplanetary
localization.


\FloatBarrier
\section{Attitude Determination System}

The Attitude Determination System (ADS) measures and estimates position and pointing direction of the payload system.
This is crucial for the further use of recorded images, as it provides the reference system and relative alignment of the taken images towards each other.

\begin{figure}
\centering
\includegraphics[width=0.8\textwidth]{figures/ADS_diagram.pdf}
\caption{Attitude Determination System overview}
\label{fig:ADS_overview}
\end{figure}

\pagebreak

\noindent
In order to produce high-accuracy attitude estimates and compensate for disadvantages of certain sensor types such as drift and noise, we chose to use a variety of sensors and fuse their information to a combined information.
The facilitated sensors will be (see figure \ref{fig:ADS_overview}):
\begin{itemize}
\item GPS receiver: Provides absolute position values, but has much high-frequency noise
\item Gyroscope: Provides accurate relative pointing direction, but has drift.
\item Accelerometer: Provides absolute pointing relative to the horizon (gravity) and linear acceleration.
\item Magnetometer: Provides absolute pointing relative to the earth's magnetic field.
\end{itemize}

\noindent
Combined all together, these sensors provide complete information about the module's attitude.

\subsection{Attitude Determination System Development}
\FloatBarrier

\begin{figure}
\centering
\includegraphics[width=0.8\textwidth]{figures/raspberry-sensors-top.jpg}
\caption[Raspberry Pi computer with self-constructed pluggable sensor shield]{Raspberry Pi computer with self-constructed pluggable sensor shield.
Mounted on the shield are gyroscope (red, left), magnetometer + accelerometer (red, top) and a GPS receiver (green, right).
Also I2C and UART 4-pin sockets are visible as well as a small voltage regulator (bottom) that connects the 3.3V GPS module to the 5V high-current source.}
\label{fig:raspberry-sensors-top}
\end{figure}

\begin{figure}
\centering
\includegraphics[width=0.8\textwidth]{figures/raspberry-sensors-open.jpg}
\caption[Raspberry Pi computer with self-constructed pluggable sensor shield]{Raspberry Pi computer with self-constructed pluggable sensor shield.
Wiring from the Raspberry Pi 26pin P1 header to individual sensors can be seen on the bottom side of the shield.}
\label{fig:raspberry-sensors-open}
\end{figure}

The attitude determination system was not only developed on the final hardware itself.
For continuation of the development during the semester break a recently released Raspberry Pi minicomputer was privately purchased, which has properties very similar to the Beagle Board and runs a derivative of the Debian operating system. We created a board to attach a complete set of sensors to the system, which is gyroscope (ITG-3200 from Sparkfun), magnetometer+accelerometer (LSM303 from Sparkfun) and GPS receiver (Venus 6 GPS Module
ST22 from SkyTraq). All sensors are pluggable and the complete system is shown in \autoref{fig:raspberry-sensors-top}. The bottom side of the board with wiring cables is shown in \autoref{fig:raspberry-sensors-open}.
This testing system does not yet contain a camera, but it will be added in the future.
\\
\\
In this configuration the whole unit consumes 3W of power and weighs 71g (30g for sensors and shield and 41g for the computer).

%% more text until end of august if I [Bastian] find time.


\subsection{Attitude Determination Algorithms}
The attitude of the sensing unit could in principle be determined by simply integrating the gyroscope rates up from an initial alignment. This however will degrade with time because of significant low-frequency drifts, sensor offsets and measurement inaccuracies.
\\
\\
To solve the problem more robustly we use a set of 3D gyro-, magnetometer and accelerometer sensors which are fused with our C++ program that runs on the board computer. Main technique is the \ac{EKF} which offers a near-optimal estimation with fixed memory usage. Attitudes are represented as quaternions, which is an elegant and cpu-effective solution.
\\
\\
Offset filtering for the magnetometer and accelerometer is done independently.
We set up an \ac{EKF} that estimates $x$- $y$- and $z$-offset and total radius along with each new measurement.
%% more text until end of august if I [Bastian] find time.
The measurements are available readily at system start-up without calibration, but the accuracy will only improve as soon as measurements from different attitudes are available. The \ac{EKF} also accounts for slow offset drifts and similar effects.
\\
\\
Estimation of the gyro offsets and rate scaling is done with combination of acceleration and magnetometer absolute attitude estimations and the gyro-integrated attitude. This outputs a gyro error value at each measurement step that is used by the Kalman algorithm to estimate the system parameters and calculate a best-estimate attitude.
%% more text until end of august if I [Bastian] find time.
\\
\\
Finally the position is determined independently from the GPS receiver. This is much simpler than fusing position values from the inertial measurement of the previously described sensors with the absolute GPS position. With the tight time constraints of the project we choose this simpler method. However in continuation of the project we want to test the fusion algorithm described in the book ``Stochastic Models -- Estimation and Control'' chapter 6.4.
%% more text until end of august if I [Bastian] find time.

\FloatBarrier
\section{Electrical Circuits}

As compared to the BeagleBone the delivered BeagleBoard's voltage at the pins
of the expansion header is only between 0~V and 1.8~V but the expected voltage
at the supply and the I2C interface for the sensors is 3.3~V. Thus additionally to
the external pull-up resistors on the I2C interface also voltage level
converters have to be used.
